{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6156556a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003768,
     "end_time": "2022-11-13T19:42:18.594607",
     "exception": false,
     "start_time": "2022-11-13T19:42:18.590839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tweet Binary Classification\n",
    "## Does a tweet indicate a real disaster?\n",
    "\n",
    "#### Methodology\n",
    "* create a vector representation of each word (using pre-trained [gensim word2vec](https://radimrehurek.com/gensim/models/woc2vec.html)\n",
    "    *  We will use the embeddings of a pre-trained model because our corpus is small.\n",
    "* average the word vectors for each tweet into a single \"sentence vector\"\n",
    "* train a binary classificatin model on the sentence vectors\n",
    "\n",
    "\n",
    "#### Reading\n",
    "* [This blogpost](https://medium.com/@dilip.voleti/classification-using-word2vec-b1d79d375381) discusses how to train a binary classification model using pretrained word vectors (from word2vec)\n",
    "* [This blogpost](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)provides a gentle introduction to the doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7e030a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T19:42:18.602509Z",
     "iopub.status.busy": "2022-11-13T19:42:18.602110Z",
     "iopub.status.idle": "2022-11-13T19:42:20.124327Z",
     "shell.execute_reply": "2022-11-13T19:42:20.123294Z"
    },
    "papermill": {
     "duration": 1.529006,
     "end_time": "2022-11-13T19:42:20.126922",
     "exception": false,
     "start_time": "2022-11-13T19:42:18.597916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import downloader\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77379176",
   "metadata": {
    "papermill": {
     "duration": 0.002969,
     "end_time": "2022-11-13T19:42:20.134687",
     "exception": false,
     "start_time": "2022-11-13T19:42:20.131718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0336b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T19:42:20.142892Z",
     "iopub.status.busy": "2022-11-13T19:42:20.142508Z",
     "iopub.status.idle": "2022-11-13T19:42:20.149368Z",
     "shell.execute_reply": "2022-11-13T19:42:20.148153Z"
    },
    "papermill": {
     "duration": 0.013858,
     "end_time": "2022-11-13T19:42:20.151773",
     "exception": false,
     "start_time": "2022-11-13T19:42:20.137915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_tweet_vector(tweet_tokens, model):\n",
    "    \n",
    "    # create a placeholder vector for aggregating our sentence\n",
    "    vec_len = len(model.get_vector(\"hello\"))\n",
    "    tweet_vec = np.zeros((1, vec_len))\n",
    "    \n",
    "    n = 0 # number of words used\n",
    "    for word in tweet_tokens:        \n",
    "        try:\n",
    "            word_vec = model.get_vector(word)\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            # if the word isn't in our vocab, represent it as zeros\n",
    "            word_vec = np.zeros((1, vec_len))\n",
    "            \n",
    "        tweet_vec += word_vec\n",
    "    \n",
    "    # make our vector sentence length invariant \n",
    "    # by converting our sum into an average\n",
    "    tweet_vec /= n\n",
    "    \n",
    "    return tweet_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d972412",
   "metadata": {
    "papermill": {
     "duration": 0.002932,
     "end_time": "2022-11-13T19:42:20.157995",
     "exception": false,
     "start_time": "2022-11-13T19:42:20.155063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read Data and pre-process our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76376ba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T19:42:20.165988Z",
     "iopub.status.busy": "2022-11-13T19:42:20.165596Z",
     "iopub.status.idle": "2022-11-13T19:43:02.976443Z",
     "shell.execute_reply": "2022-11-13T19:43:02.975582Z"
    },
    "papermill": {
     "duration": 43.576581,
     "end_time": "2022-11-13T19:43:03.737733",
     "exception": false,
     "start_time": "2022-11-13T19:42:20.161152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "X_train shape: (7613, 25)\n",
      "y_train shape: (7613,)\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained word2vec model\n",
    "model_gt25 = downloader.load(\"glove-twitter-25\")\n",
    "\n",
    "# data\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "\n",
    "\n",
    "# ---- Pre-process our documents ---- \n",
    "# tokenize tweets\n",
    "train_df[\"tweet_tokens\"] = train_df[\"text\"].apply(simple_preprocess)\n",
    "test_df[\"tweet_tokens\"] = train_df[\"text\"].apply(simple_preprocess)\n",
    "\n",
    "\n",
    "# create document vectors\n",
    "train_df[\"tweet_vector\"] = train_df[\"tweet_tokens\"].apply(lambda tt: make_tweet_vector(tt, model=model_gt25))\n",
    "test_df[\"tweet_vector\"] = test_df[\"tweet_tokens\"].apply(lambda tt: make_tweet_vector(tt, model=model_gt25))\n",
    "\n",
    "\n",
    "# create numpy arrays for model training and testing\n",
    "# For now, lets just use text. We can add keywords latter\n",
    "X_train = np.concatenate(train_df[\"tweet_vector\"].values)\n",
    "X_test = np.concatenate(test_df[\"tweet_vector\"].values)\n",
    "\n",
    "y_train = train_df[\"target\"].to_numpy()\n",
    "\n",
    "# make sure that our data dimensions make sense\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889e7b9",
   "metadata": {
    "papermill": {
     "duration": 0.759168,
     "end_time": "2022-11-13T19:43:05.201538",
     "exception": false,
     "start_time": "2022-11-13T19:43:04.442370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Use `GridSearchCV` to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6dba7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T19:43:06.666999Z",
     "iopub.status.busy": "2022-11-13T19:43:06.666635Z",
     "iopub.status.idle": "2022-11-13T19:47:55.941167Z",
     "shell.execute_reply": "2022-11-13T19:47:55.939587Z"
    },
    "papermill": {
     "duration": 290.749807,
     "end_time": "2022-11-13T19:47:56.656773",
     "exception": false,
     "start_time": "2022-11-13T19:43:05.906966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-2,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [5, 10, 15],\n",
       "                         'max_features': [5, 10],\n",
       "                         'min_samples_leaf': [2, 5, 15, 20],\n",
       "                         'n_estimators': [500]},\n",
       "             scoring='f1', verbose=100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'max_features': [5, 10],\n",
    "    'min_samples_leaf': [2, 5, 15, 20],\n",
    "    'n_estimators': [500]}\n",
    "\n",
    "n_jobs = -2\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier() # Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf,param_grid=param_grid, \n",
    "                           scoring=\"f1\", cv=3, n_jobs=n_jobs, verbose=100)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b04a00",
   "metadata": {
    "papermill": {
     "duration": 0.708796,
     "end_time": "2022-11-13T19:47:58.150105",
     "exception": false,
     "start_time": "2022-11-13T19:47:57.441309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Generate Test Set Predictions\n",
    "\n",
    "Retrain `RandomForestClassifier` using the best hyperparameters but with the _full_ training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c35ad4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T19:47:59.619779Z",
     "iopub.status.busy": "2022-11-13T19:47:59.619382Z",
     "iopub.status.idle": "2022-11-13T19:48:14.443287Z",
     "shell.execute_reply": "2022-11-13T19:48:14.442176Z"
    },
    "papermill": {
     "duration": 15.531241,
     "end_time": "2022-11-13T19:48:14.445745",
     "exception": false,
     "start_time": "2022-11-13T19:47:58.914504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_full_input training score: 0.9235518192565348\n"
     ]
    }
   ],
   "source": [
    "rf_full_input = RandomForestClassifier(**grid_search.best_params_)\n",
    "rf_full_input.fit(X_train, y_train)\n",
    "training_score = rf_full_input.score(X_train, y_train)\n",
    "print(\"rf_full_input training score: {}\".format(training_score))\n",
    "\n",
    "# get predictions\n",
    "y_pred = rf_full_input.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8617b",
   "metadata": {
    "papermill": {
     "duration": 0.722936,
     "end_time": "2022-11-13T19:48:15.926413",
     "exception": false,
     "start_time": "2022-11-13T19:48:15.203477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798bb479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T19:48:17.395033Z",
     "iopub.status.busy": "2022-11-13T19:48:17.394458Z",
     "iopub.status.idle": "2022-11-13T19:48:17.410913Z",
     "shell.execute_reply": "2022-11-13T19:48:17.409870Z"
    },
    "papermill": {
     "duration": 0.72515,
     "end_time": "2022-11-13T19:48:17.413014",
     "exception": false,
     "start_time": "2022-11-13T19:48:16.687864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format data to required output\n",
    "preds_df = pd.DataFrame()\n",
    "preds_df[\"id\"] = test_df[\"id\"]\n",
    "preds_df[\"target\"] = y_pred\n",
    "\n",
    "# Save as csv\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "preds_df.to_csv(submission_path,\n",
    "                header=True, \n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182a4af",
   "metadata": {
    "papermill": {
     "duration": 0.696718,
     "end_time": "2022-11-13T19:48:18.861268",
     "exception": false,
     "start_time": "2022-11-13T19:48:18.164550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 370.948154,
   "end_time": "2022-11-13T19:48:22.236590",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-13T19:42:11.288436",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
